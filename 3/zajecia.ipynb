{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analiza i preprocessing danych\n",
    "\n",
    "Organizator: Koło naukowe BioMedicalAI  \n",
    "![biomedical.svg](biomedical.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analiza danych (20m)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn.datasets\n",
    "\n",
    "# 1) Wstępna analiza \n",
    "# Opis datasetu (https://scikit-learn.org/1.5/datasets/toy_dataset.html#wine-recognition-dataset)\n",
    "# NOTE: wiedza dziedzinowa - \n",
    "dataset  = sklearn.datasets.load_wine(as_frame=True)[\"frame\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ilość próbek\n",
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kolumny, typy danych i ilość pustych wartości\n",
    "dataset.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Przykładowe dane z przodu\n",
    "dataset.head(n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Przykładowe dane z końca datasetu i porównanie czy dane różnią się mocno pomiędzy przodem a końcem zbioru danych. \n",
    "dataset.tail(n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statystyki danych numeryczych\n",
    "dataset.describe().apply(lambda x: x.apply('{0:.5f}'.format))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wizualizacja poszczególnych kolumn na wykresach\n",
    "for column in dataset.columns:\n",
    "    sns.boxplot(dataset, y=column)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Badanie zbalansowania danych\n",
    "sns.histplot(dataset, x=\"target\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wizualizacja poszczególnych kolumn na wykresach\n",
    "for column in dataset.drop(\"target\", axis=1).columns:\n",
    "    sns.kdeplot(dataset, x=column, hue=\"target\", palette=\"tab10\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pairplot\n",
    "sns.pairplot(dataset, hue='target', palette=\"tab10\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heatmapa korelacji między kolumnami\n",
    "plt.figure(figsize = (10,10))\n",
    "sns.heatmap(dataset.drop(\"target\", axis=1).corr(), annot = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing (30m)\n",
    "Preprocessing ma na celu przystosowanie danych pod dalsze ich użycie przez człowieka lub model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sklearn.preprocessing\n",
    "import sklearn.metrics\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Podział kolumn na kolumny wejściowe i wyjściowe\n",
    "y = dataset[\"target\"].to_numpy()\n",
    "X = dataset.drop(\"target\", axis=1).to_numpy()\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Podział na dane treningowe, walidacyjne i testowe\n",
    "# Dane treningowe - na tych danych będziemy trenować model\n",
    "# Dane walidacyjne - na tych danych będziemy dopasowywać model (hiperparementry modelu etc)\n",
    "# Dane testowe - na tych danych będziemy testować ostateczną jakośc modelu\n",
    "# Zazwyczaj dane dzieli się w stosunku 70%:10%:20%\n",
    "# Innym sposobem może być zastosowanie cross-validacji w celu redukcji wpływu losowego podziału na ostateczne wyniki działania modelu\n",
    "# https://medium.com/@masadeghi6/how-to-split-your-data-for-machine-learning-eae893a8799c\n",
    "seed = 21\n",
    "\n",
    "X_tv, X_test, y_tv, y_test = train_test_split(X, y, test_size=0.2, random_state=seed)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_tv, y_tv, test_size=0.25, random_state=seed) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zadania a metryki\n",
    "| Zadanie | Funkcja straty | Metryki |\n",
    "|-|-|-|\n",
    "| Regresja | MSE, MAE | R2, MSE, MAE, RMSE |\n",
    "| Klasyfikacja | CE, KL Divergence | Dokładność(Accuracy), Precyzja(Precision), Czułość(Recall), F1, ROC_AUC, MCC |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy - jaki procent przypadków raka oraz braku raka poprawnie wykryliśmy\n",
    "\n",
    "$Accuracy = \\frac{TP+TN}{TP+TN+FP+FN}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Precision - jaki procent wykrytych przypadków raka jest faktycznie rakiem\n",
    "\n",
    "$Precision = \\frac{TP}{TP+FP}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "F1 - średnia harmoniczna precision i recall\n",
    "\n",
    "$F1 = \\frac{2*Precision*Recall}{Precision+Recall} = \\frac{2*TP}{2*TP+FP+FN}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ROC_AUC - jak zmienia się działanie klasyfikatora w zależności od przyjętego progu klasyfikacji"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Matthews’s Correlation Coefficient (MCC) - metryka jakości klasyfikatora odporniejsza na niezbalansowane datasety\n",
    "\n",
    "$MCC=TP×TN−FP×FN(TP+FP)(TP+FN)(TN+FP)(TN+FN)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Krok 1. Bez przetwarzania danych wejściowych\n",
    "# Ważnym elementem jest poprawność metryk wykorzystywanych w procesie oceniania jakości modelu.\n",
    "# W tym celu warto wykorzystać metryki dostarczane przez biblioteki.\n",
    "\n",
    "seed = 21\n",
    "best_model = None\n",
    "best_size = None \n",
    "best_score = 0\n",
    "for size in [(100,), (100, 50), (50, 50, 10), (100, 100, 100, 100)]:\n",
    "    model = MLPClassifier(hidden_layer_sizes=size, random_state=seed, max_iter=2000)\n",
    "    model.fit(X_train, y_train)\n",
    "    val_score = sklearn.metrics.accuracy_score(y_val, model.predict(X_val))\n",
    "    test_score = sklearn.metrics.accuracy_score(y_test, model.predict(X_test))\n",
    "    print(\"VAL + TEST score\", size, round(val_score), round(test_score))\n",
    "    if val_score > best_score:\n",
    "        best_score = val_score\n",
    "        best_model = model\n",
    "        best_size = size\n",
    "\n",
    "# Model wybrany na podstawie danych walidacyjych wcale nie musi być najlepszy (!)\n",
    "print(\"TEST score based on VAL score\", best_size, round(sklearn.metrics.accuracy_score(y_test, best_model.predict(X_test))))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.vstack([y_test, best_model.predict(X_test)]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Raport z klasyfikacji\n",
    "print(sklearn.metrics.classification_report(y_test, best_model.predict(X_test), zero_division=np.nan))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Micro a macro metryka\n",
    "\n",
    "$Precision = \\frac{TP}{TP+FP}$\n",
    "\n",
    "$Precision_{micro} = \\frac{(TP_0+TP_1+TP_2)}{(TP_0+TP_1+TP_2)+(FP_0+FP_1+FP_2)}$\n",
    "\n",
    "$Precision_{macro} = \\frac{Precision_0 + Precision_1 + Precision_2}{3}$\n",
    "\n",
    "Makro uśrednianie uśrednia pomiędzy klasami, micro uśrednianie uśrednia pomiędzy elementami"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Macierz pomyłek\n",
    "sklearn.metrics.confusion_matrix(y_test, best_model.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Macierz pomyłek\n",
    "# NOTE: Określanie błędów modelu i środki zaradcze\n",
    "sns.heatmap(sklearn.metrics.confusion_matrix(y_test, best_model.predict(X_test)), annot=True, cmap=\"Blues\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing - scalers https://scikit-learn.org/dev/auto_examples/preprocessing/plot_all_scaling.html#original-data\n",
    "# NOTE: Data leakage\n",
    "\n",
    "seed = 21\n",
    "best_model = None\n",
    "best_scaler = None \n",
    "best_score = 0\n",
    "sc = sklearn.preprocessing.StandardScaler()\n",
    "\n",
    "\n",
    "for scaler in [\n",
    "    sklearn.preprocessing.StandardScaler,\n",
    "    sklearn.preprocessing.MinMaxScaler,\n",
    "    sklearn.preprocessing.RobustScaler,\n",
    "    sklearn.preprocessing.PowerTransformer,\n",
    "    sklearn.preprocessing.Normalizer,\n",
    "]:\n",
    "    sc = scaler()\n",
    "    X_train_transformed = sc.fit_transform(X_train)\n",
    "    X_val_transformed = sc.transform(X_val)\n",
    "    X_test_transformed = sc.transform(X_test)\n",
    "\n",
    "    model = MLPClassifier(hidden_layer_sizes=(100, 50), random_state=seed, max_iter=2000)\n",
    "    model.fit(X_train_transformed, y_train)\n",
    "    val_score = sklearn.metrics.accuracy_score(y_val, model.predict(X_val_transformed))\n",
    "    test_score = sklearn.metrics.accuracy_score(y_test, model.predict(X_test_transformed))\n",
    "    print(\"VAL + TEST score\", scaler, round(val_score, 2), round(test_score, 2))\n",
    "    if val_score > best_score:\n",
    "        best_score = val_score\n",
    "        best_model = model\n",
    "        best_scaler = scaler\n",
    "\n",
    "\n",
    "bsc = best_scaler()\n",
    "X_train_transformed = bsc.fit_transform(X_train)\n",
    "X_val_transformed = bsc.transform(X_val)\n",
    "X_test_transformed = bsc.transform(X_test)\n",
    "print(\"TEST score based on VAL score\", best_scaler, round(sklearn.metrics.accuracy_score(y_test, best_model.predict(X_test_transformed))))\n",
    "print(sklearn.metrics.classification_report(y_test, best_model.predict(X_test_transformed), zero_division=np.nan))\n",
    "print(round(sklearn.metrics.matthews_corrcoef(y_test, best_model.predict(X_test_transformed))))\n",
    "sns.heatmap(sklearn.metrics.confusion_matrix(y_test, best_model.predict(X_test_transformed)), annot=True, cmap=\"Blues\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dyskretyzacja danych\n",
    "\n",
    "seed = 21\n",
    "sc = sklearn.preprocessing.KBinsDiscretizer(n_bins=5)\n",
    "X_train_transformed = sc.fit_transform(X_train)\n",
    "X_val_transformed = sc.transform(X_val)\n",
    "X_test_transformed = sc.transform(X_test)\n",
    "\n",
    "model = MLPClassifier(hidden_layer_sizes=(100, 50), random_state=seed, max_iter=2000)\n",
    "model.fit(X_train_transformed, y_train)\n",
    "val_score = sklearn.metrics.accuracy_score(y_val, model.predict(X_val_transformed))\n",
    "test_score = sklearn.metrics.accuracy_score(y_test, model.predict(X_test_transformed))\n",
    "print(\"VAL + TEST score\", round(val_score), round(test_score))\n",
    "\n",
    "print(sklearn.metrics.classification_report(y_test, model.predict(X_test_transformed), zero_division=np.nan))\n",
    "sns.heatmap(sklearn.metrics.confusion_matrix(y_test, model.predict(X_test_transformed)), annot=True, cmap=\"Blues\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OneHotEncoding w celu uproszenia klasyfikacji modelom NN\n",
    "one_hot_y_train = sklearn.preprocessing.OneHotEncoder().fit_transform(y_train.reshape(-1, 1))\n",
    "one_hot_y_train.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC AUC dla gorszego modelu\n",
    "model = MLPClassifier(hidden_layer_sizes=(100, 50, 10), random_state=seed, max_iter=2000)\n",
    "model.fit(X_train, y_train)\n",
    "label_binarizer = sklearn.preprocessing.OneHotEncoder().fit(y_train.reshape(-1, 1))\n",
    "y_onehot_test = label_binarizer.transform(y_test.reshape(-1, 1)).toarray()\n",
    "y_score = model.predict_proba(X_test)\n",
    "\n",
    "display = sklearn.metrics.RocCurveDisplay.from_predictions(\n",
    "    y_onehot_test.ravel(),\n",
    "    y_score.ravel(),\n",
    "    name=\"micro-average OvR\",\n",
    "    color=\"darkorange\",\n",
    "    plot_chance_level=True,\n",
    ")\n",
    "_ = display.ax_.set(\n",
    "    xlabel=\"False Positive Rate\",\n",
    "    ylabel=\"True Positive Rate\",\n",
    "    title=\"Micro-averaged One-vs-Rest\\nReceiver Operating Characteristic\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-validation ze stratyfikacją po klasie\n",
    "scores = []\n",
    "\n",
    "external_skf = sklearn.model_selection.StratifiedKFold(n_splits=5)\n",
    "for tv, test in external_skf.split(X, y):\n",
    "    internal_skf = sklearn.model_selection.StratifiedKFold(n_splits=5)\n",
    "    for train, val in internal_skf.split(X[tv], y[tv]):\n",
    "        seed = 21\n",
    "        sc = sklearn.preprocessing.StandardScaler()\n",
    "        X_train_transformed = sc.fit_transform(X[train])\n",
    "        X_val_transformed = sc.transform(X[val])\n",
    "        X_test_transformed = sc.transform(X[test])\n",
    "\n",
    "        model = MLPClassifier(hidden_layer_sizes=(100, 50), random_state=seed, max_iter=2000)\n",
    "        model.fit(X_train_transformed, y[train])\n",
    "        val_score = sklearn.metrics.matthews_corrcoef(y[val], model.predict(X_val_transformed))\n",
    "        test_score = sklearn.metrics.matthews_corrcoef(y[test], model.predict(X_test_transformed))\n",
    "        scores.append(test_score)\n",
    "        print(\"VAL score\", round(val_score, 2))\n",
    "\n",
    "np_scores = np.array(scores)\n",
    "print(np_scores.min().round(2), np_scores.mean().round(2), np_scores.max().round(2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ankieta\n",
    "![\"Ankieta\"](./ankieta.png)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backpropagation step by step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([[1, 2]])\n",
    "\n",
    "w_1 = np.array([[1.0, 2.0], [3.0, 4.0]])\n",
    "b_1 = np.array([[5.0, 6.0]])\n",
    "w_2 = np.array([[7.0], [8.0]])\n",
    "b_2 = np.array([[9.0]])\n",
    "\n",
    "weights = np.array([[4], [-1]])\n",
    "bias = np.array([0.5])\n",
    "lr = 0.001\n",
    "y_true = x @ weights + bias\n",
    "\n",
    "for i in range(100):\n",
    "    y_1 = (x @ w_1 + b_1)\n",
    "    y_2 = (y_1 @ w_2 + b_2)\n",
    "    y_pred = y_2\n",
    "\n",
    "    error = 2. * (y_pred - y_true) / len(x)\n",
    "    e_3 = error\n",
    "\n",
    "    wg_2 = y_1.T @ e_3\n",
    "    wb_2 = np.sum(e_3, axis=0)\n",
    "    e_1 = e_3 @ w_2.T\n",
    "    w_2 -= wg_2 * lr\n",
    "    b_2 -= wb_2 * lr\n",
    "\n",
    "\n",
    "    wg_1 = x.T @ e_1\n",
    "    wb_1 = np.sum(e_1, axis=0)\n",
    "    e_0 = e_1 @ w_1.T\n",
    "    w_1 -= wg_1 * lr\n",
    "    b_1 -= wb_1 * lr\n",
    "\n",
    "\n",
    "\n",
    "    print(y_1, y_2)\n",
    "    print(\"W2\")\n",
    "    print(e_3)\n",
    "    print(wg_2)\n",
    "    print(wb_2)\n",
    "    print(\"W1\")\n",
    "    print(e_1)\n",
    "    print(wg_1)\n",
    "    print(wb_1)\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

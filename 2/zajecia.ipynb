{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wprowadzenie do sieci neuronowych\n",
    "\n",
    "Organizator: Koło naukowe BioMedicalAI  \n",
    "![biomedical.svg](biomedical.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Idea i historia sieci neuronowych (30m)\n",
    "Sieci neuronowe są podzbiorem technik maszynowego uczenia, stosowane w obszarach wizji komputerowej, przetwarzania języka naturalnego, generacji tekstu i obrazów itp. Sieć zazyczaj skłąda się z kilku warstw: warstwy wejściowej, N warstw ukrytych oraz warstwy wyjścia. Pomiędzy warstwami stosuje się nieliniowe funkcje aktywancji w celu modelowania nieliniowego zachowania. Sieci trenowane są zazwyczaj z wykorzystaniem mechanizmu propagacji wstecznej, wykorzystującej pochodne funkcji w celu propagowania błedu oraz zmiany parametrów.\n",
    "\n",
    "![\"Schemat sieci neuronowej\"](./The-Fully-Connected-Neural-Network-model-used-in-our-paper_W640.jpg)  \n",
    "*Dumor, Koffi & Li, Yao. (2019). Estimating China’s Trade with Its Partner Countries within the Belt and Road Initiative Using Neural Network Analysis. Sustainability. 11. 1449. 10.3390/su11051449.*\n",
    "\n",
    "### Historia sieci neuronowych\n",
    "Historia sieci neuronowych bardzo mocno splata się z historią psychologii oraz poznania mechanizów stojących za ludzkim postrzeganiem oraz podejmowaniem decyzji.  \n",
    "\n",
    "Koncept sieci neuronowych pojawia się już w 1943r. w artykule \"A logical calculus of the ideas immanent in nervous activity\" W. McCulloch, W. Pitts, gdzie próbowano zamodelować działanie mózgu poprzez proste elementy logiczne.  \n",
    "\n",
    "Dużym osiagnięciem bylo opublikowanie w 1958r. artykułu \"The perceptron: A probabilistic model for information storage and organization in the brain.\" Franka Rosenblatta opisujący system przetwarzania informacji wizyjnej - mark I Perceptron.   \n",
    "System zbudowany był w oparciu o maszynę IBM 704 i składał się z 3 warstw:\n",
    "* sensory units (S-units) - warstwa wejściowa, 20x20 fotodetektorów, gdzie każdy S-unit łączył się losowo z warstwą A-unitów\n",
    "* association units (A-units) - warstwa ukryta składająca się z 512 neuronów, ustawiana ręcznie poprzez odpowiednie ustawienie potencjometrów\n",
    "* response units(R-units) - warstwa wyjścia\n",
    "Badane było wykorzystanie Perceptrona do analizy zdjęć lotniczych i detekcji systemów wojskowych\n",
    "\n",
    "![\"Wykorzystanie systemu Perceptron\"](./perceptron-use.jpg)\n",
    "*By National Museum of the U.S. Navy - 330-PSA-80-60 (USN 710739), Public Domain, https://commons.wikimedia.org/w/index.php?curid=70710209*\n",
    "\n",
    "![\"Schemat opisujący elementy Perceptrona\"](./percpetron-op-man.png)  \n",
    "*By John C. Hay, Albert E. Murray - https://apps.dtic.mil/sti/tr/pdf/AD0236965.pdf, Public Domain, https://commons.wikimedia.org/w/index.php?curid=143176022*\n",
    "\n",
    "![\"Figura z artykułu F. Rosenblatta porównująca działanie ludzkiego mózgu z perceptronem do przetwarzania widzenia.\"](./Organization_of_a_biological_brain_and_a_perceptron.png)  \n",
    "*By Rosenblatt, F. - Rosenblatt, F. The Design of an Intelligent Automaton, Research Reviews, Office of Naval Research. Washington, October 1958, 5-13, Public Domain, https://commons.wikimedia.org/w/index.php?curid=139658945*\n",
    "\n",
    "W 1974 r. Paul Werbos w swojej pracy doktorskiej opisał użycie propagacji wstecznej w celu uczenia sieci neuronowej, natomiast w 1985r. David Rumelhart wraz z  Geoffrey Hintonem oraz Ronald J. Williamsem niezaleznie opisali zastosowanie algorytmu propagacji wstecznej w celu uczenia MLP.\n",
    "\n",
    "Sepp Hochreiter w 1995 r. zaproponował LSTM (long short-term memory) jako rozwiązanie problemu zanikającego gradientu w rekurencyjnych sieciach neuronowych.\n",
    "\n",
    "W 1980 r. Kunihiko Fukushima zaproponowal CNN bazując na badaniach z lat 50-60 opisujących sposób postrzegania przez koty. Warta wspomnienia jest praca Yann LeCun, który w 1989 r. zaproponował sieć LeNet bazującą na CNN do odczytu odręcznego pisma, co spowodowało budowę komercyjnych zastosowań opartych na sieciach neuronowych i CNN.\n",
    "\n",
    "Głębokie uczenie określane jako uczenie wielowarstwowe dużych sieci zaczęło się rozpowszechniać wraz z zastsoowaniem GPU do przyśpieszenia uczenia. W 2009 r. Raina, Madhavan, oraz Andrew Ng pracowali nad sieciami trenowanymi z użyciem GPU. W 2012 r. głęboka sieć AlexNet znacząco poprawiła najwyższy wynik w konkursie ImageNet, co uposzechniło głębokie uczenie.\n",
    "\n",
    "W 2017r. w artykule \"Attention Is All You Need\" Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, Lukasz Kaiser, and Illia Polosukhin zaproponowano architekturę transformer, rewolucjonizującą przetwarzanie sekwencji (NLP). \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorytm propagacji wstecznej (5m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib tk\n",
    "from matplotlib import animation\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "n_steps = 100\n",
    "x_lin = np.linspace(-2, 2, 400)\n",
    "y_lin = np.linspace(-2, 2, 400)\n",
    "x, y = np.meshgrid(x_lin, y_lin)\n",
    "positions = np.zeros((n_steps+1, 2))\n",
    "lr = 0.1\n",
    "\n",
    "# Funkcja którą optymalizujemy\n",
    "def func(x, y):\n",
    "    return x**2 - y**2\n",
    "\n",
    "# Nasz model składający się z 2 parametrów\n",
    "theta_x, theta_y = np.random.normal(size=(2))\n",
    "positions[0, 0] = theta_x\n",
    "positions[0, 1] = theta_y\n",
    "\n",
    "for epoch in range(n_steps):\n",
    "    grad_x = 2 * theta_x\n",
    "    grad_y = -2 * theta_y\n",
    "\n",
    "    theta_x -= lr * grad_x\n",
    "    theta_y -= lr * grad_y\n",
    "\n",
    "    positions[epoch+1, 0] = theta_x\n",
    "    positions[epoch+1, 1] = theta_y\n",
    "\n",
    "# Animation of SGD steps\n",
    "fig, ax = plt.subplots()\n",
    "ax.set_aspect('equal')\n",
    "\n",
    "scat = ax.scatter(positions[0, 0], positions[0, 1], color=\"red\")\n",
    "line, = ax.plot(positions[0, 0], positions[0, 1], color=\"red\")\n",
    "\n",
    "plt.imshow(func(x, y), extent=[-2, 2, 2, -2])\n",
    "\n",
    "def animate(i):\n",
    "    line.set_data(positions[:i+1, 0], positions[:i+1, 1])\n",
    "    scat.set_offsets([positions[i, 0], positions[i, 1]])\n",
    "    plt.draw()\n",
    "\n",
    "anim = animation.FuncAnimation(fig, animate, frames=n_steps+1, interval=100)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optymalizatory (10m)\n",
    "Optymalizatory pozwalają na bardziej skomplikowane mechanizmy zmiany parametrów np. poprzez zachowanie momentu. Naczęściej używane Adam, RMSProp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib tk\n",
    "from matplotlib import animation\n",
    "import torch\n",
    "\n",
    "n_steps = 10\n",
    "x_lin = torch.linspace(-2, 2, 1000)\n",
    "y_lin = torch.linspace(-2, 2, 1000)\n",
    "x, y = torch.meshgrid(x_lin, y_lin)\n",
    "\n",
    "def func(x, y):\n",
    "    return x**2 - y**2\n",
    "\n",
    "optimizers_cls = [\n",
    "    torch.optim.SGD,\n",
    "    torch.optim.Adam,\n",
    "    torch.optim.RMSprop\n",
    "]\n",
    "\n",
    "theta_init = torch.rand((2)) * 2 - 1\n",
    "optim_theta = [theta_init.clone().requires_grad_() for _ in optimizers_cls]\n",
    "optimizers = [opti_cls([theta], lr=0.1) for theta, opti_cls in zip(optim_theta, optimizers_cls)]\n",
    "positions = torch.zeros((len(optimizers_cls), n_steps+1, 2))\n",
    "\n",
    "# Training loop\n",
    "positions[:, 0] = theta_init.detach()\n",
    "\n",
    "\n",
    "for epoch in range(n_steps):\n",
    "    for i, (opti, theta) in enumerate(zip(optimizers, optim_theta)):\n",
    "        opti.zero_grad()\n",
    "        output = -1 * func(theta[0], theta[1])\n",
    "        output.backward()\n",
    "        opti.step()\n",
    "        positions[i, epoch+1] = theta.detach()\n",
    "\n",
    "# Animation of SGD steps\n",
    "fig, ax = plt.subplots()\n",
    "ax.set_aspect('equal')\n",
    "\n",
    "scat = [None] * len(optimizers)\n",
    "line = [None] * len(optimizers)\n",
    "for i, opti in enumerate(optimizers):\n",
    "    scat[i] = ax.scatter(positions[i, 0, 0], positions[i, 0, 1])\n",
    "    line[i], = ax.plot(positions[i, 0, 0], positions[i, 0, 1], label=opti)\n",
    "\n",
    "plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "plt.imshow(func(x, y), extent=[-2, 2, 2, -2])\n",
    "\n",
    "def animate(frame):\n",
    "    for i, _ in enumerate(optimizers):\n",
    "        line[i].set_data(positions[i, :frame+1, 0], positions[i, :frame+1, 1])\n",
    "        scat[i].set_offsets([positions[i, frame, 0], positions[i, frame, 1]])\n",
    "    plt.draw()\n",
    "\n",
    "anim = animation.FuncAnimation(fig, animate, frames=n_steps+1, interval=100)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funkcje aktywacji (5m)\n",
    "Funkcje aktywacji określają wyściowy poziom aktywacji neuronu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Liniowa\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def relu_activation(x):\n",
    "    return x\n",
    "\n",
    "inputs = np.linspace(-10, 10, 50)\n",
    "outputs = np.vectorize(relu_activation)(inputs)\n",
    "\n",
    "plt.plot(inputs, outputs)\n",
    "plt.title('Liniowa funkcja aktywacji')\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('Y')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sigmoida\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def relu_activation(x):\n",
    "    return 1 / (1 + np.e ** -x)\n",
    "\n",
    "inputs = np.linspace(-10, 10, 50)\n",
    "outputs = np.vectorize(relu_activation)(inputs)\n",
    "\n",
    "plt.plot(inputs, outputs)\n",
    "plt.title('Sigmoidalna funkcja aktywacji')\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('Y')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ReLU\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def relu_activation(x):\n",
    "    return x if x >= 0.0 else 0.0\n",
    "\n",
    "inputs = np.linspace(-10, 10, 51)\n",
    "outputs_relu = np.vectorize(relu_activation)(inputs)\n",
    "\n",
    "plt.plot(inputs, outputs_relu)\n",
    "plt.title('Rectifier Linear Unit')\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('Y')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Softmax\n",
    "import numpy as np\n",
    "\n",
    "def softmax(x):\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / e_x.sum()\n",
    "\n",
    "scores = np.array([3.0, 1.0, 0.2])\n",
    "print(softmax(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funkcje straty (10m)\n",
    "\n",
    "* Regresja\n",
    "  * MSE - średni błąd kwadratowy $1/n * \\sum{(x - y) ^ 2}$\n",
    "  * MAE - średni błąd absolutny $1/n * \\sum{|x - y|}$\n",
    "\n",
    "* Klasyfikacja\n",
    "  * CE - cross entropy  $-\\sum_{c=1}^My_{o,c}\\log(p_{o,c})$ gdzie M to liczba klas\n",
    "  * NLL - negative loglikelihood $NLL(y) = -{\\log(p(y))}$\n",
    "  * KL divergence - Kullback-Leibler Divergence $KL(\\hat{y} || y) = \\sum_{c=1}^{M}\\hat{y}_c \\log{\\frac{\\hat{y}_c}{y_c}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zbudujmy własną sieć (15m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear:\n",
    "  def __init__(self, input_dim: int, num_hidden: int = 1):\n",
    "    self.weights = np.random.randn(input_dim, num_hidden) * np.sqrt(2. / input_dim)\n",
    "    self.bias = np.zeros(num_hidden)\n",
    "  \n",
    "  def __call__(self, x):\n",
    "    self.x = x\n",
    "    output = x @ self.weights + self.bias\n",
    "    return output\n",
    "\n",
    "  def backward(self, gradient, lr):\n",
    "    # Calculate gradients\n",
    "    self.weights_gradient = self.x.T @ gradient\n",
    "    self.bias_gradient = np.sum(gradient, axis=0)\n",
    "    self.x_gradient = gradient @ self.weights.T\n",
    "\n",
    "    # Apply update\n",
    "    self.weights = self.weights - lr * self.weights_gradient\n",
    "    self.bias = self.bias - lr * self.bias_gradient\n",
    "    return self.x_gradient\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MSE:\n",
    "  def __call__(self, y_pred, y_true):\n",
    "    self.y_pred = y_pred\n",
    "    self.y_true = y_true\n",
    "    return np.mean((y_pred - y_true) ** 2)\n",
    "\n",
    "  def backward(self):\n",
    "    n = self.y_true.shape[0]\n",
    "    self.gradient = 2. * (self.y_pred - self.y_true) / n\n",
    "    return self.gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__(self, layers: list):\n",
    "        self.layers = layers\n",
    "\n",
    "    def __call__(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "    \n",
    "    def backward(self, gradient, lr):\n",
    "        for layer in self.layers[::-1]:\n",
    "            gradient = layer.backward(gradient, lr)\n",
    "        return gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_3d(x, y, y_pred=None):\n",
    "  fig = plt.figure()\n",
    "  ax = fig.add_subplot(111, projection='3d')\n",
    "  ax.scatter(x[:, 0], x[:, 1], y, label='base function')\n",
    "  if y_pred is not None:\n",
    "    ax.scatter(x[:, 0], x[:, 1], y_pred, label='our function')\n",
    "  plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generacja liniowego datasetu\n",
    "sample_size = 100\n",
    "dimensions = 2\n",
    "x = np.random.uniform(-1.0, 1.0, (sample_size, dimensions))\n",
    "\n",
    "weights = np.array([[4, -1]]).T\n",
    "bias = np.array([0.5])\n",
    "y = x @ weights + bias\n",
    "plot_3d(x, y[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epoch = 100\n",
    "lr = 0.1\n",
    "model = Model([\n",
    "    Linear(2)\n",
    "])\n",
    "loss = MSE()\n",
    "\n",
    "plot_3d(x, y[:, 0], model(x)[:, 0])\n",
    "\n",
    "for epoch in range(100):\n",
    "    y_pred = model(x)\n",
    "    loss_val = loss(y_pred, y)\n",
    "    gradient_from_loss = loss.backward()\n",
    "    model.backward(gradient_from_loss, lr)\n",
    "    print(loss_val)\n",
    "\n",
    "plot_3d(x, y[:, 0], model(x)[:, 0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generacja nie liniowego datasetu\n",
    "sample_size = 100\n",
    "dimensions = 2\n",
    "x = np.random.uniform(-1.0, 1.0, (sample_size, dimensions))\n",
    "\n",
    "a = np.array([[7, -1]]).T\n",
    "b = np.array([[3, 1]]).T\n",
    "bias = np.array([7])\n",
    "y = (x ** 2) @ a + x @ b + bias\n",
    "plot_3d(x, y[:, 0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epoch = 100\n",
    "lr = 0.1\n",
    "model = Model([\n",
    "    Linear(2)\n",
    "])\n",
    "loss = MSE()\n",
    "\n",
    "plot_3d(x, y[:, 0], model(x)[:, 0])\n",
    "\n",
    "for epoch in range(num_epoch):\n",
    "    y_pred = model(x)\n",
    "    loss_val = loss(y_pred, y)\n",
    "    gradient_from_loss = loss.backward()\n",
    "    model.backward(gradient_from_loss, lr)\n",
    "    print(loss_val)\n",
    "\n",
    "plot_3d(x, y[:, 0], model(x)[:, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLU:\n",
    "    def __call__(self, input_):\n",
    "        self.input_ = input_\n",
    "        self.output = np.clip(self.input_, 0, None)\n",
    "        return self.output\n",
    "    \n",
    "    def backward(self, output_gradient, _):\n",
    "      self.input_gradient = (self.input_ > 0) * output_gradient\n",
    "      return self.input_gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epoch = 100\n",
    "lr = 0.1\n",
    "model = Model([\n",
    "    Linear(2, 2),\n",
    "    ReLU(),\n",
    "    Linear(2),\n",
    "])\n",
    "loss = MSE()\n",
    "\n",
    "plot_3d(x, y[:, 0], model(x)[:, 0])\n",
    "\n",
    "for epoch in range(num_epoch):\n",
    "    y_pred = model(x)\n",
    "    loss_val = loss(y_pred, y)\n",
    "    gradient_from_loss = loss.backward()\n",
    "    model.backward(gradient_from_loss, lr)\n",
    "    print(loss_val)\n",
    "\n",
    "plot_3d(x, y[:, 0], model(x)[:, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sigmoid:\n",
    "    def __call__(self, input):\n",
    "        self.output =  1 / (1  + np.exp(-input))\n",
    "        return self.output\n",
    "    \n",
    "    def backward(self, output_gradient, _):\n",
    "      return output_gradient *self.output * (1-self.output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epoch = 100\n",
    "lr = 0.1\n",
    "model = Model([\n",
    "    Linear(2, 2),\n",
    "    Sigmoid(),\n",
    "    Linear(2),\n",
    "])\n",
    "loss = MSE()\n",
    "\n",
    "plot_3d(x, y[:, 0], model(x)[:, 0])\n",
    "\n",
    "for epoch in range(num_epoch):\n",
    "    y_pred = model(x)\n",
    "    loss_val = loss(y_pred, y)\n",
    "    gradient_from_loss = loss.backward()\n",
    "    model.backward(gradient_from_loss, lr)\n",
    "    print(loss_val)\n",
    "\n",
    "plot_3d(x, y[:, 0], model(x)[:, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ankieta\n",
    "![\"Ankieta\"](./ankieta.png)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dodatkowe linki\n",
    "[Wizualizacja algorytmow optymalizacji](https://imgur.com/a/visualizing-optimization-algos-Hqolp#NKsFHJb)  \n",
    "[Wizualizacja crossentropy i loglikelihood](https://towardsdatascience.com/cross-entropy-negative-log-likelihood-and-all-that-jazz-47a95bd2e81)  \n",
    "[3Brown1Blue - propagacja wsteczna](https://www.youtube.com/watch?v=tIeHLnjs5U8)  \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
